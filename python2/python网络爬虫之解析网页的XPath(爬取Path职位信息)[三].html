<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修python网络爬虫之解析网页的XPath(爬取Path职位信息)[三]' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>python网络爬虫之解析网页的XPath(爬取Path职位信息)[三]</center></div><div class='banquan'>原文出处:本文由博客园博主陌陌卡上提供。<br/>
原文连接:https://www.cnblogs.com/liwangwang/p/12106614.html</div><br>
    <div class="toc">
    <p class="toc-title">目录</p>
    <div class="toc-list">
        <ul>
        <li><a href="#前言">前言</a><ul>
        <li><a href="#xpath的使用方法">XPath的使用方法</a></li>
        <li><a href="#xpath爬取数据">XPath爬取数据</a></li>
        </ul></li>
        <li><a href="#后言">后言</a></li>
        </ul>
    </div>
</div>
<p>@(目录)</p>
<h1 id="前言">前言</h1>
<p>本章同样是解析网页，不过使用的解析技术为XPath。</p>
<p>相对于之前的BeautifulSoup,我感觉还行，也是一个比较常用的一种解析方式<br />
，<br />
并且更加的符合我们之前的一个逻辑思维，不过看情况吧，看各位准备怎么用吧。</p>
<h2 id="xpath的使用方法">XPath的使用方法</h2>
<p>同样的先下载<strong>lxml插件</strong>,并且导入里面的etree</p>
<p><img src="./images/python网络爬虫之解析网页的XPath(爬取Path职位信息)[三]0.png" alt="在这里插入图片描述" /></p>
<pre><code><code>&quot;&quot;&quot;

XPath的学习
&quot;&quot;&quot;
from lxml import etree



# 案例文件
html_doc = &quot;&quot;&quot;
&lt;div&gt;
    &lt;ul&gt;
        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;www.baidu.com&quot;&gt;baidu&lt;/a&gt;
        &lt;li class=&quot;item-1 one&quot; name=&quot;first&quot;&gt;&lt;a href=&quot;https://blog.csdn.net/qq_25343557&quot;&gt;myblog&lt;/a&gt;
        &lt;li class=&quot;item-1 two&quot; name=&quot;first&quot;&gt;&lt;a href=&quot;https://blog.csdn.net/qq_25343557&quot;&gt;myblog2&lt;/a&gt;
        &lt;li class=&quot;item-2&quot;&gt;&lt;a href=&quot;https://www.csdn.net/&quot;&gt;csdn&lt;/a&gt;
        &lt;li class=&quot;item-3&quot;&gt;&lt;a href=&quot;https://hao.360.cn/?a1004&quot;&gt;bbb&lt;/a&gt;
        &lt;li class=&quot;aaa&quot;&gt;&lt;a href=&quot;https://hao.360.cn/?a1004&quot;&gt;aaa&lt;/a&gt;
&quot;&quot;&quot;

html = etree.HTML(html_doc)

# 1、获取所有li下的所有a标签
print(html.xpath(&quot;//li/a&quot;))

#2、获取指定的li标签item-0
print(html.xpath(&quot;//li[@class=&#39;item-0&#39;]&quot;))

#3、获取指定的li标签item-0下面的a标签
print(html.xpath(&quot;//li[@class=&#39;item-0&#39;]/a&quot;))

#4、获取指定的li标签item-0下面的a标签里面的内容
print(html.xpath(&quot;//li[@class=&#39;item-0&#39;]/a/text()&quot;))

# 高级进阶用法
# 1、匹配属性以什么类型开头的class(starts-with())
print(html.xpath(&quot;//li[starts-with(@class,&#39;item-&#39;)]&quot;))

# 2、匹配里面所有相同的item-1,(contains())
print(html.xpath(&quot;//li[contains(@class,&#39;item-1&#39;)]&quot;))

# 3、多属性的匹配（and）
print(html.xpath(&quot;//li[contains(@class,&#39;one&#39;) and contains(@name,&#39;first&#39;)]/a/text()&quot;))


# 4、按顺序来排序
# 第2个
print(html.xpath(&quot;//li[2]/a/text()&quot;))
# 最后一个
print(html.xpath(&quot;//li[last()]/a/text()&quot;))
# 最后一个-1个
print(html.xpath(&quot;//li[last()-1]/a/text()&quot;))
# 小于等于3的序号li
print(html.xpath(&quot;//li[position()&lt;=3]/a/text()&quot;))</code></pre>
<hr />
<h2 id="xpath爬取数据">XPath爬取数据</h2>
<p><img src="./images/python网络爬虫之解析网页的XPath(爬取Path职位信息)[三]1.png" alt="在这里插入图片描述" /></p>
<p><img src="./images/python网络爬虫之解析网页的XPath(爬取Path职位信息)[三]2.png" alt="在这里插入图片描述" /></p>
<pre><code><code>&quot;&quot;&quot;
案例：爬取《51job》相关职位信息，并保存成cvs文件格式
&quot;&quot;&quot;
import requests
from lxml import etree
import csv
import time

headers = {
    &quot;User-Agent&quot;: &quot;Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14&quot;
}
f = open(&quot;Python职位.csv&quot;, &quot;w&quot;, newline=&quot;&quot;)
writer = csv.writer(f)
writer.writerow([&#39;编号&#39;, &#39;职位名称&#39;, &#39;公司名称&#39;, &#39;薪资&#39;, &#39;地址&#39;, &#39;发布时间&#39;])

i = 1
for page in range(1, 159):
    response = requests.get(f&quot;https://search.51job.com/list/020000,000000,0000,00,9,99,python,2,{page}.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=&quot;, headers=headers)
    response.encoding = &quot;gbk&quot;
    if response.status_code == 200:
        html = etree.HTML(response.text)
        els = html.xpath(&quot;//div[@class=&#39;el&#39;]&quot;)[4:]
        for el in els:
            jobname = str(el.xpath(&quot;p[contains(@class,&#39;t1&#39;)]/span/a/@title&quot;)).strip(&quot;[&#39;]&quot;)
            jobcom = str(el.xpath(&quot;span[@class=&#39;t2&#39;]/a/@title&quot;)).strip(&quot;[&#39;]&quot;)
            jobaddress = str(el.xpath(&quot;span[@class=&#39;t3&#39;]/text()&quot;)).strip(&quot;[&#39;]&quot;)
            jobmoney = str(el.xpath(&quot;span[@class=&#39;t4&#39;]/text()&quot;)).strip(&quot;[&#39;]&quot;)
            jobdate = str(el.xpath(&quot;span[@class=&#39;t5&#39;]/text()&quot;)).strip(&quot;[&#39;]&quot;)
            writer.writerow([i, jobname, jobcom, jobaddress, jobmoney, jobdate])
            i +=1
    print(f&quot;第{page}页获取完毕&quot;)
</code></pre>
<h1 id="后言">后言</h1>
<p>多学一种解析网页的方式多一种选择</p>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>