<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修【Python爬虫案例学习】下载某图片网站的所有图集' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>【Python爬虫案例学习】下载某图片网站的所有图集</center></div><div class='banquan'>原文出处:本文由博客园博主嗨学编程提供。<br/>
原文连接:https://www.cnblogs.com/Pythonmiss/p/11311588.html</div><br>
    <h4 id="前言">前言</h4>
<p>其实很简短就是利用爬虫的第三方库Requests与BeautifulSoup。<br />
其实就几行代码，但希望没有开发基础的人也能一下子看明白，所以大神请绕行。</p>
<h4 id="基本环境配置">基本环境配置</h4>
<ul>
<li>python 版本：2.7</li>
<li><p>IDE ：pycharm</p>
<h4 id="相关模块">相关模块</h4></li>
</ul>
<pre><code><code>import urllib2
import io
import random
import urllib
from bs4 import BeautifulSoup
import re
import os</code></pre>
<h4 id="完整代码">完整代码</h4>
<pre><code><code>import urllib2
import io
import random
import urllib
from bs4 import BeautifulSoup
import re
import os

import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
&#39;&#39;&#39;
遇到不懂的问题？Python学习交流群：821460695满足你的需求，资料都已经上传群文件，可以自行下载！
&#39;&#39;&#39;
def getHtml(url):
    #尽可能让爬虫显示为一个正常用户。若不设置，则发送的请求中，user-agent显示为Python+版本
    user_agent = [
        &#39;Mozilla/5.0 (Windows NT 5.2) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.122 Safari/534.30&#39;,
        &#39;Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0&#39;,
        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.2; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)&#39;,
        &#39;Opera/9.80 (Windows NT 5.1; U; zh-cn) Presto/2.9.168 Version/11.50&#39;,
        &#39;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/533.21.1 (KHTML, like Gecko) Version/5.0.5 Safari/533.21.1&#39;,
        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)&#39;        
    ]
    #设置网页编码格式，解码获取到的中文字符
    encoding = &quot;gb18030&quot;
    #构造http请求头，设置user-agent
    header = {&quot;User-Agent&quot;:random.choice(user_agent)}
    #构造发送请求
    request = urllib2.Request(url,headers=header)
    #发送请求，获取服务器响应回来的html页面
    html = urllib2.urlopen(request).read()
    #使用beautifulSoup处理的html页面，类似dom
    soup = BeautifulSoup(html,from_encoding=encoding)
    return soup

# 获取整个站点所有图集的页码
def getPageNum(url):
    soup = getHtml(url)
    # 直接在站点首页获取所有图集的总页码
    nums=soup.find_all(&#39;a&#39;,class_=&#39;page-numbers&#39;)
    # 除掉“下一页”的链接，并获取到最后一页
    totlePage = int(nums[-2].text)
    return totlePage

#获取指定页面下图集名称和链接
def getPicNameandLink(url):
    
    soup = getHtml(url)
    meun = []
    #类似html dom对象，直接查找id为“pins”的ul标签，返回的结果是一个dom对象
    targetul = soup.find(&quot;ul&quot;,id=&quot;pins&quot;)
    if targetul:    
        #获取该ul下所有的超链接，返回值的类型是list，find_all中第二个参数表示某个指定标签的属性
        pic_list = targetul.find_all(&quot;a&quot;,target=&quot;_blank&quot;)
        if pic_list:
           # 遍历所有指定的标签a
            for pic in pic_list:
                #获取图集的链接
                link = pic[&quot;href&quot;]
                picturename = &quot;&quot;
                #找到标签a中，“class”为“lazy”的img标签。
                #find中，第二个参数表示某个指定标签的属性。
                #在python中class是保留字，所有标签的class属性的名称为“class_”
                img = pic.find(&quot;img&quot;,class_=&#39;lazy&#39;)
                if img:
                    # 保证中文字符能够正常转码。
                    picturename = unicode(str(img[&quot;alt&quot;]))
                else:
                    continue
                #插入图集名称和对应的url
                meun.append([picturename,link])    
        
        return meun
    return None

#function获取所有的图集名称
def getallAltls(url):
    totalpage = getPageNum(url)
    #获取首页中所有的图集名称。首页的url和其他页面不同，没有page
    meun = getPicNameandLink(url)
    #循环遍历所有的图集页面，获取图集名称和链接
    for pos in range(2,totalpage):
        currenturl = url + &quot;/page/&quot; + str(pos)
        #getPicNameandLink()返回的值是一个list。
        #当一个list插入到另一个list中时，使用extend。
        #若是插入一个值时，可以用append
        meun.extend(getPicNameandLink(currenturl))
    
    return meun
    
# 获取从首页到指定页面所有的图集名称和链接
def getparAltls(url,page):
    meun = getPicNameandLink(url)
        
    for pos in range(2,page):
        currenturl = url + &quot;/page/&quot; + str(pos)
        meun.extend(getPicNameandLink(currenturl))
        
    return meun

#获取单个相册内图片页码
def getSinglePicNum(url):
    soup = getHtml(url)
    #pagenavi还是一个对象（Tag），可以通过find_all找出指定标签出来
    pagenavi = soup.find(&quot;div&quot;,class_=&quot;pagenavi&quot;)
    pagelink = pagenavi.find_all(&quot;a&quot;)
    
    num = int(pagelink[-2].text)
    return num


#下载单个相册中的所有图片
def getSinglePic(url,path):
    totalPageNum = getSinglePicNum(url)
    #从第一页开始，下载单个图集中所有的图片
    #range()第二个参数是范围值的上限，循环时不包括该值
    #需要加1以保证读取到所有页面。
    for i in range(1,totalPageNum + 1):
        currenturl = url + &quot;/&quot; + str(i)
        downloadpic(currenturl,path)
       
#下载单个页面中的图片 
def downloadpic(url,path):
    soup = getHtml(url)
    #找出指定图片所在父容器div
    pageimg = soup.find(&quot;div&quot;,class_=&quot;main-image&quot;)
    
    if pageimg:
        #找出该div容器中的img，该容器中只有一个img
        img = pageimg.find(&quot;img&quot;)
        #获取图片的url
        imgurl = img[&quot;src&quot;]
        #获取图片的文件名
        restring = r&#39;[A-Za-z0-9]+\.jpg&#39;
        reimgname = re.findall(restring,imgurl)
        
        #将图片保存在指定目录下
        path = str(path)
        if path.strip() == &quot;&quot;:
            downloadpath = reimgname[0]
        else:
            downloadpath = path + &quot;/&quot; + reimgname[0]
        #伪装一下下载的http请求，否则有些站点不响应下载请求。
        #不设置的话，下载请求中的user-agent为python+版本号
        urllib.URLopener.version = &#39;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 SE 2.X MetaSr 1.0&#39;
        #下载图片到指定目录中，保留图片在服务器上的文件名
        urllib.urlretrieve(imgurl,downloadpath)
    
def downimgofsite(url,path = &quot;&quot;):
    
    path = str(path)
    #获取所有图集的名称和链接
    meun_list = getallAltls(url)
    directorypath = &quot;&quot;
    
    for meun in meun_list:
        directoryname = meun[0]
        if path.strip() != &quot;&quot;:
            directorypath = path + &quot;/&quot; + directoryname
        else:
            directorypath = os.getcwd + &quot;/&quot; + directoryname
        
        if not os.path.exists(directorypath):
            os.makedirs(directorypath)
        
        getSinglePic(meun[1], directorypath)
        

if __name__ == &quot;__main__&quot;:
   
    
   # page = 8
    url = &quot;XXXXX&quot;
    menu = getallAltls(url)
    #menu = getparAltls(url, page)  
    
    f = open(&quot;tsts.txt&quot;,&quot;a&quot;)
    for i in menu:
        f.write(str(unicode(i[0]))+&quot;\t&quot;+str(i[1])+&quot;\n&quot;)
    f.close()</code></pre>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>