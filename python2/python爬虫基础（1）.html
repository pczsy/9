<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修python爬虫基础（1）' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>python爬虫基础（1）</center></div><div class='banquan'>原文出处:本文由博客园博主五木徒羚提供。<br/>
原文连接:https://www.cnblogs.com/toling/p/11300207.html</div><br>
    <ul>
<li><strong><span style="font-size: 14px;">请求头常见参数</span></strong></li>
</ul>
<pre><code><span style="font-size: 14px;">
在http协议中,向服务器发送一个请求,数据分为三个部分,第一个是把数据放在url中,第二个是把数据放在body中(在post请
求中),第三个就是把数据放在header中。这里介绍在网络爬虫中经常会用到的一些请求头参数:</span><br /><span style="font-size: 14px;">
1. <strong>User-Agent</strong>:浏览器名称,这个在网络爬虫中经常会被使用到。请求一个两页的时候,服务器通过这个参数就可以知道这个请求
是由哪种览器发送的。如果我们是通过爬虫发送请求,那么我们的User-Agent就是 Python,这对于那些有反爬虫机制的网站来
说,可以轻易的判断你这个请求是爬虫,因此我们要经常设贸这个值力一些刻览器的值,来伪装我们的爬虫</span><br /><span style="font-size: 14px;">
2. <strong>Referer</strong>:表明当的这个请求是从哪个ur1过来的,这个一般也可以用来做反爬虫技术,如果不是从指定页面过来的,那么就不
做相关的响应</span><br /><span style="font-size: 14px;">
3. <strong>Cookie</strong>:http协议是无状态的。也就是同一个人发送了两次请求,服务器没有能力知道这两个请求是否来自同一个人&middot;因此这
时候就用 cookie来做标识,一般如果想要做登录后才能访问的网站,那么就需要发送cookie信息了</span></pre>
<ul>
<li><span style="font-size: 14px;"><strong>常见响应状态码</strong></span></li>
</ul>
<p><span style="font-size: 14px;">1.<strong>200</strong>:请求正常,服务器正常的返回数据</span></p>
<p><span style="font-size: 14px;">2.<strong>301</strong>:永久重定问。比加在访问w.jangdong.com的时候会重定向到w.Jd.cms</span></p>
<p><span style="font-size: 14px;">3.<strong>302</strong>:临时重定向比如在访问一个需要置录的页面的时候,而此时没有登录,那么就会重定问到登录页面</span></p>
<p><span style="font-size: 14px;">4.<strong>400</strong>:请求的w1在服务器上找不到。换句话说就是请求ur1错误</span></p>
<p><span style="font-size: 14px;">5.<strong>403</strong>:服务器托缩访问,权限不够</span></p>
<p><em id="__mceDel"><em id="__mceDel"><span style="font-size: 14px;">6.<strong>500</strong>:服务器内部错误。可能是服务器出现bug了</span></em></em></p>
<p>&nbsp;</p>
<ul>
<li><span style="font-size: 14px;"><strong>urllib库</strong> urllib库是 Python中一个最基本的网络请求库。可以模拟浏览器的行为,向指定的服务器发送一个请求,并可以保存服务器返回的 数据</span></li>
<li><span style="font-size: 14px;"><strong>urlopen函数:</strong></span></li>
</ul>
<p><span style="font-size: 14px;"> 在Phon3的urllib库中,所有和网络请求相关的方法,都楼集到urllib, request模块下面了,以先来看下 urlopen基本的 使用:</span></p>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">from</span><span style="color: #000000;"> urllib inport request
resp.reqvest.urlopen(</span><span style="color: #800000;">"</span><span style="color: #800000;">http://www.baidu.com</span><span style="color: #800000;">"</span><span style="color: #000000;">)
</span><span style="color: #0000ff;">print</span>(resp. reado)</pre>
</div>
<p><span style="font-size: 14px;"> 实际上,使用浏览器访问百度,右键查看源代码,你会发现,跟我们刚才打印出来的数据是一模一样的。也就是说,上面的三行代码 就已经帮我们把百度的百页的全部代码爬下来了。一个基本的url请求对应的 python代码真的非常简单 以下对 urlopen函数的进行详细讲解: </span></p>
<p><span style="font-size: 14px;"><strong>1.url</strong>:请求的url </span></p>
<p><span style="font-size: 14px;"><strong>2.data</strong>:请求的dta,如果设网了这个值,那么将变成post请求 </span></p>
<p><span style="font-size: 14px;"><strong>3.返回值</strong>:返回值是一个对象,这个对象是一个类文件句柄对象 有read(sze)、 readline、readine以及 getcode等方法</span></p>
<p>&nbsp;</p>
<ul>
<li><span style="font-size: 14px;"><strong>urlretrieve函数</strong></span></li>
</ul>
<pre><code><span style="font-size: 14px;">这个的数可以方便的将网页上的一个文件保存到本地。以下代码可以菲常方便的将百度的百页下载到本地:</span></pre>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">from</span> urllsb <span style="color: #0000ff;">import</span><span style="color: #000000;"> request
request.urlretrieve(</span><span style="color: #800000;">'</span><span style="color: #800000;">http://www.baiducom/,</span><span style="color: #800000;">'</span>baidu.html<span style="color: #800000;">'</span><span style="color: #800000;">)</span></pre>
</div>
<ul>
<li><strong><span style="font-size: 14px;">urlencode函数:</span></strong></li>
</ul>
<p><span style="font-size: 14px;"> 用决览器发送请求的时候,如果url中包含了中文或者其他特殊字符,那么浏览器会自动的给我们进行编码。而如果使用代码发送请 求,那么就必须手动的进行编码,这时候就应该使用urlencode来实现, urlencode可把字典数据转换为uRL编码的数据 示例代码如下:</span></p>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">from</span><span style="color: #000000;"> urllib inport parse

data </span>= {&ldquo;name&rdquo;:&ldquo;爬虫&rdquo;<span style="color: #800000;">'</span><span style="color: #800000;">, &ldquo;greet&rdquo;:"he1 lo word&rdquo;, &ldquo;age&rdquo;:100}</span>
qs. =<span style="color: #000000;"> parse.urlencode (data)
</span><span style="color: #0000ff;">print</span>(qs)</pre>
</div>
<p><span style="font-size: 14px;"> parse_qs函数 可以将经过编码后的u参数进行解码。</span></p>
<p>&nbsp;</p>
<ul>
<li><span style="font-size: 14px;"><strong>ProxyHandler处理器(代理设置)</strong></span></li>
</ul>
<p><span style="font-size: 14px;">很多网站会检测某一段时间某个iP的访问次数(通过凌量统计,系统日志等),如果访问次数多的不像正常人,它会禁止这个iP的访问 所以我们可以设置一些代理服务器,每隔一段时间换一个代理,就算iP被禁止,依然可以换个iP续爬取 urllib中通过 ProxyHandler来设置使用代理服务器,下面代码说明如何使用自定义opener来使用代理: 这个是没有使用代理的</span></p>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">from</span> urllib <span style="color: #0000ff;">import</span><span style="color: #000000;"> request

</span><span style="color: #008000;">#</span><span style="color: #008000;"> 这个是没有使用代理的</span><span style="color: #008000;">
#</span><span style="color: #008000;"> resp = request.urlopen("http://baidu.com")</span><span style="color: #008000;">
#</span><span style="color: #008000;"> print(resq.read().decode("utf-8"))</span>

<span style="color: #008000;">#</span><span style="color: #008000;"> 这个是使用代理的</span>
handler = request.ProxyHander({<span style="color: #800000;">"</span><span style="color: #800000;">http</span><span style="color: #800000;">"</span>:<span style="color: #800000;">"</span><span style="color: #800000;">218.66.82:32512</span><span style="color: #800000;">"</span><span style="color: #000000;">})

opener </span>=<span style="color: #000000;"> request.build_opener(handler)
req </span>= request.Request(<span style="color: #800000;">"</span><span style="color: #800000;">http://www.baidu.com</span><span style="color: #800000;">"</span><span style="color: #000000;">)
resp </span>=<span style="color: #000000;"> opener.open(req)
</span><span style="color: #0000ff;">print</span>(resp.read())</pre>
</div>
<p>&nbsp;</p>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>